{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b8ee6d",
   "metadata": {},
   "source": [
    "**GRADIENT DESCENT** via NumPy y PyTorch: entendiendo mejor el algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003ff31",
   "metadata": {},
   "source": [
    "El descenso de gradiente (gradient descent) es la estrategia de optimización más popular utilizada en machine learning (deep learning). Se utiliza cuando se entrenan modelos con datos y es ampliamente usado ya que se puede combinar con otro tipo de algoritmos.\n",
    "\n",
    "Los datos de entrenamiento ayudan a los modelos a aprender con el tiempo, y la función de costo (cost function) dentro del descenso de gradiente actúa específicamente como un barómetro, midiendo su precisión con cada iteración de actualizaciones de parámetros. Hasta que la función sea cercana o igual a cero, el modelo continuará ajustando sus parámetros para producir el menor error posible. Una vez que los modelos de machine learning se optimizan para la precisión, pueden ser herramientas poderosas para aplicaciones de inteligencia artificial (IA) y ciencias de la computación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc506292",
   "metadata": {},
   "source": [
    "Por otro lado vale la pena destacar a los **gradientes** los cuales miden cuánto cambia la salida (el output) de una función si cambias un poco las entradas (inputs). En otras palabras, simplemente mide el cambio en todos los pesos (weights) con respecto al cambio en el error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d480de",
   "metadata": {},
   "source": [
    "Hay tres tipos populares de descenso de gradiente que difieren principalmente en la cantidad de datos que utilizan:\n",
    "- Batch Gradiente Descent (descenso de gradiente por lotes): calcula el error para cada ejemplo dentro del conjunto de datos de entrenamiento, pero solo después de que se hayan evaluado todos los ejemplos de entrenamiento, el modelo se actualiza. Todo este proceso es como un ciclo y se llama \"training epoch\".\n",
    "- Stochastic Gradient Descent (SGD): hace el descenso de gradiente para cada ejemplo de entrenamiento dentro un conjunto de datos. Esto significa que actualiza los parámetros para cada ejemplo de entrenamiento uno por uno. Dependiendo del problema, esto puede hacer que SGD sea más rápido que el descenso de gradiente por lotes. Una ventaja es que las actualizaciones frecuentes nos permiten tener una tasa de mejora bastante detallada.\n",
    "- Mini-Batch Gradiente Descent: es el método de acceso ya que es una combinación de los conceptos de SGD y descenso de gradiente por lotes. Simplemente divide el conjunto de datos de entrenamiento en lotes pequeños y realiza una actualización para cada uno de esos lotes. Esto crea un equilibrio entre la solidez del descenso de gradiente estocástico y la eficiencia del descenso de gradiente por lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33de4a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b9493a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediccion antes de entrenar: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "Prediccion despues del entrenamiento: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# Implementacion de regresion lineal con numpy de forma MANUAL\n",
    "# Funcion de Combinaciones lineares de nuestros weight con inputs ==> f = w * x\n",
    "# no se tienen bias en cuenta\n",
    "# ejemplo = f = 2 * x\n",
    "\n",
    "# Regresion Lineal\n",
    "# f = w * x \n",
    "\n",
    "# aca : f = 2 * x\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model output\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE (mean square error o error cuadratico medio) Funcion de Coste\n",
    "# mide el error  en problemas de regresion\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "# J = MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2x(w*x - y)\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.dot(2*x, y_pred - y).mean()\n",
    "\n",
    "print(f'Prediccion antes de entrenar: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Learning rate o ratio de aprendizaje\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediccion = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # calculando los gradientes\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    # actualizacion de weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "     \n",
    "print(f'Prediccion despues del entrenamiento: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3200b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45930cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediccion antes de entrenar: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediccion despues del entrenamiento: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# Implementacion de Regresion lineal con Pytorch de forma \"AUTOMATICA\"\n",
    "# Se reemplazan los gradientes calculados manualmente con autograd\n",
    "\n",
    "# Regresion Lineal\n",
    "# f = w * x \n",
    "\n",
    "# aca : f = 2 * x\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model output\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE (mean square erro o error cuadratico medio) Funcion de coste \n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "print(f'Prediccion antes de entrenar: f(5) = {forward(5).item():.3f}')\n",
    "\n",
    "# Learning rate o ratio de aprendizaje\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediccion = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # calculando gradientes = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # actualizacion de weights\n",
    "    #w.data = w.data - learning_rate * w.grad\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    # zero los gradientes despues de la actualizacion\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
    "\n",
    "print(f'Prediccion despues del entrenamiento: f(5) = {forward(5).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c74c920",
   "metadata": {},
   "source": [
    "La **función de costo (o pérdida)** mide la diferencia o error entre el **y real** y **y prediccion** en su posición actual. Esto mejora la **eficacia del modelo de aprendizaje automático** al proporcionar retroalimentación al modelo para que pueda ajustar los parámetros para minimizar el error y encontrar el mínimo local o global. Itera continuamente, moviéndose a lo largo de la dirección del descenso más pronunciado (o el gradiente negativo) hasta que la función de costo esté cerca o en cero. En este punto, el modelo dejará de aprender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee91968f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
