{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1571345",
   "metadata": {},
   "source": [
    "En el corazón de todas las redes neuronales en PyTorch se encuentra el paquete **autograd**. Comencemos con una breve introducción.\n",
    "\n",
    "El paquete **autograd** proporciona diferenciación automática para todas las **operaciones en tensores**. Es un marco definido en tiempo de ejecución, lo que significa que la retropropagación se basa en su código para determinar cómo comportarse, y cada iteración puede ser diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20afd9a1",
   "metadata": {},
   "source": [
    "Su incidencia en los tensores:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67248cc9",
   "metadata": {},
   "source": [
    "**torch.Tensor** es la clase central de este paquete. Si la condicion **.requires_grad** se establece en **True**, se rastrearán todas las operaciones en este tensor. Cuando se completa el cálculo, todos los gradientes se calculan automáticamente llamando a **.backward()** y todos los gradientes de este tensor se acumularán automáticamente en la propiedad **.grad.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d175c85",
   "metadata": {},
   "source": [
    "Para evitar que un tensor realice un seguimiento del historial, puede llamar al método **.detach()** para separarlo del historial de cálculo y deshabilitar el seguimiento de sus cálculos futuros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26fe35",
   "metadata": {},
   "source": [
    "De igual forma el seguimiento del historial (y el uso de la memoria) de un bloque de código se puede envolver con **torch.no_grad():**. Lo anterior es especialmente útil al evaluar modelos, ya que estos pueden tener parámetros entrenables con **require_grad=True**, pero no necesitamos cálculos de gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6383d148",
   "metadata": {},
   "source": [
    "**Tensor** y **Function** están interconectados y construyen un gráfico acíclico que codifica una historia completa de computación. Cada tensor tiene un atributo **.grad_fn** que hace referencia a una función que ha creado **Tensor** (excepto los tensores creados por el usuario: cuando su **grad_fn** es **None**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a047753",
   "metadata": {},
   "source": [
    "Si necesita calcular la derivada, puede llamar a **.backward()** en **Tensor**. Si **Tensor** es un escalar (es decir, contiene datos de un elemento), no necesita especificar ningún argumento **backward()** pero si tiene más elementos, debe especificar un argumento de gradiente (**gradient**) para que coincida con la forma del tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26157c0",
   "metadata": {},
   "source": [
    "**Para tener en cuenta**:\n",
    "en otros artículos, puede ver que **Tensor** está envuelto en **Variable** para proporcionar un cálculo de gradiente automático. El llamado de **Variable** se marcó como caducado en la versión 0.41. Ahora puede usar **Tensor** directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8669fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae2562",
   "metadata": {},
   "source": [
    "Creando un tensor y configurandolo como **require_grad=True** para rastrear su historial de cálculo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ad31a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d3225",
   "metadata": {},
   "source": [
    "- Operar con tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5636ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcfa43a",
   "metadata": {},
   "source": [
    "El resultado **y** se ha calculado, por lo que **grad_fn** se ha generado automáticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4790e1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x00000169B84D0940>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8937e4ba",
   "metadata": {},
   "source": [
    "Ahora realizar una operacion en **y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c00d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa7fdba",
   "metadata": {},
   "source": [
    "*.requires_grad()*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501568c5",
   "metadata": {},
   "source": [
    "**.requires_grad_(...)** puede cambiar la propiedad **require_grad** de un tensor existente. Si no se especifica, el indicador de entrada predeterminado es **False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81f3233e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x00000169BCC29E50>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c844765",
   "metadata": {},
   "source": [
    "# Gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceff4ea",
   "metadata": {},
   "source": [
    "Backpropagation **out** debido a que es un escalar ==> **out.backward()** es igual a **out.backward(torch.tensor(1))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1d6e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2540b181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5c3a727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-194.4764, 1357.0542, 1086.9559], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61085520",
   "metadata": {},
   "source": [
    "En la celda anterior **y** ya no es un escalar. **torch.autograd** no puede calcular directamente el jacobiano completo(vector-jacobian product), pero si solo queremos el producto vector-jacobiano, simplemente pasamos el vector como parámetro hacia atrás **backward:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e421224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00b2b1",
   "metadata": {},
   "source": [
    "Si **.requires_grad=True** pero no desea realizar cálculos de **autograd** puede envolver la variable con **torch.no_grad():**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d561de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a819ba",
   "metadata": {},
   "source": [
    "Para profundizar conceptos de **autograd**: https://pytorch.org/docs/autograd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74916dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
