{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e15ea6",
   "metadata": {},
   "source": [
    "## Redes Neuronales con PyTorch:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2121657",
   "metadata": {},
   "source": [
    "Las redes neuronales se construyen utilizando el paquete **torch.nn**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85994b5b",
   "metadata": {},
   "source": [
    "Teniendo una idea de autograd, **nn** depende de este paquete para definir los modelos y diferenciarlos. Un **nn.Module** contiene capas y un método **forward(input)** que devuelve la salida (output)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c01eaa",
   "metadata": {},
   "source": [
    "##### Un procedimiento de entrenamiento típico para una red neuronal es el siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff6fd7d",
   "metadata": {},
   "source": [
    " - Definir la red neuronal que tiene algunos parámetros aprendibles (pesos/weights)\n",
    " - Iterar sobre un conjunto de datos de entrada (inputs)\n",
    " - Procesar la entrada(input) a través de la red\n",
    " - Calcular la pérdida o loss (qué tan lejos estan los outputs de ser correctos)\n",
    " - Propagar los gradientes de vuelta a los parámetros de la red\n",
    " - Actualizar los pesos (weights) de la red, generalmente usando una reglas de actualizacion: **weigth = weight - learning_rate * gradient**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1631962d",
   "metadata": {},
   "source": [
    "### 1. Definiendo una Red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4435fbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 canal imagen input, 6 canales output, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # una operacion afín: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max agrupacion sobre una ventana (2, 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # Si el tamaño es un cuadrado, solo se puede especificar un número\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # todas las dimensiones excepto la dimension del batch\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486e864",
   "metadata": {},
   "source": [
    "**torch.nn** solo admite mini lotes (batches). Todo el paquete **torch.nn** solo permite entradas que son un mini lote de muestras y NO una sola muestra. Por ejemplo en **nn.Conv2d** se tomará un tensor 4D de ``nSamples x nChannels x Height x Width``. Si tiene una sola muestra, simplemente se usa ``input.unsqueeze(0)`` para agregar una dimensión de lote falsa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f67a60",
   "metadata": {},
   "source": [
    "Solo tiene que definir la función **forward**. La función **backward** (donde se calculan los gradientes) se define automáticamente mediante **autograd**. Puede usar cualquiera de las operaciones de **Tensor** en la función **forward**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a73bfea",
   "metadata": {},
   "source": [
    "Los parámetros que se pueden aprender de un modelo son devueltos por **net.parameters()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ea5ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d6a75",
   "metadata": {},
   "source": [
    "Ahora probemos con una entrada aleatoria de 32x32. \n",
    "\n",
    "A Tener en cuenta: El tamaño de entrada esperado para esta red (LeNet) es 32x32. Por ejemplo para usar esta red en el conjunto de datos MNIST, cambie el tamaño de las imágenes del conjunto de datos a 32x32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34531e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0098, -0.0274, -0.0671, -0.0148,  0.0986,  0.0164, -0.1319, -0.1066,\n",
      "         -0.0745, -0.1136]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d777b02e",
   "metadata": {},
   "source": [
    "Ahora **Zero** los **gradient buffers** de todos los parámetros y **backprops** con gradientes aleatorios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef399bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eee6dae",
   "metadata": {},
   "source": [
    "#### Para que todo quede mas claro:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7021578",
   "metadata": {},
   "source": [
    "- **torch.Tensor** - una array multidimensional con soporte para operaciones con autograd como lo puede ser **backward()**. También contiene el gradiente w.r.t. del tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8438a5b8",
   "metadata": {},
   "source": [
    "- **nn.Module** - Módulo como tal de las redes neuronales. Manera conveniente para encapsular parámetros con asistentes para moverlos a GPU, exportar, cargar, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eaac8b",
   "metadata": {},
   "source": [
    "- **nn.Parameter** - un tipo de tensor que se registra automáticamente como parámetro cuando se asigna como atributo a un modulo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42095f46",
   "metadata": {},
   "source": [
    "- **autograd.Function** - implementa definiciones hacia adelante(forward) y hacia atrás(backward) de una operación autograd. Cada operación de **Tensor** crea al menos un solo nodo de funcion que se conecta a las funciones que crearon un Tensor y codifica su historial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ae9c71",
   "metadata": {},
   "source": [
    "## 2. Funcion de pérdida o Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cc9ecf",
   "metadata": {},
   "source": [
    "Una función de pérdida toma el par (output, target) de un input y calcula un valor que estima qué tan lejos está el output del target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05900aca",
   "metadata": {},
   "source": [
    "Hay varias funciones de pérdida diferentes en el paquete **nn**. Una pérdida simple es: **nn.MSELoss**, que calcula el error cuadrático medio entre la entrada (input) y el objetivo (target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abaa2fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7860, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a un target cualquiera(random)\n",
    "target = target.view(1, -1)  # que tenga la misma forma (shape) del output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffdb7ab",
   "metadata": {},
   "source": [
    "Ahora si sigue la pérdida en la dirección hacia atrás, utilizando su atributo **.grad_fn** verá un gráfico de cálculos similar a este:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140e19dd",
   "metadata": {},
   "source": [
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e326e",
   "metadata": {},
   "source": [
    "Entonces cuando llamamos a **loss.backward()** todo el gráfico diferencia la pérdida w.r.t., y todos los Tensores en el gráfico que tiene **require_grad=True** tendrán su Tensor **.grad** acumulado con el gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c43c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x0000018EF57DBA00>\n",
      "<AddmmBackward object at 0x0000018EF57DB730>\n",
      "<AccumulateGrad object at 0x0000018EF57DBA00>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c00260",
   "metadata": {},
   "source": [
    "## 3. Backprop (propagación de gradientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70f6786",
   "metadata": {},
   "source": [
    "Para \"backpropagar\" el error todo lo que tenemos que hacer es **loss.backward().** Sin embargo, se deben borrar los gradientes existentes; de lo contrario, los gradientes se acumularán con los ya existentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc94ace",
   "metadata": {},
   "source": [
    "Ahora llamaremos a **loss.backward()** y echaremos un vistazo a los gradientes **bias** de conv1 antes y después del **backward.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b6c6861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0170,  0.0029, -0.0093, -0.0015, -0.0016,  0.0023])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes los gradientes buffers de todos los parametros\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782b128",
   "metadata": {},
   "source": [
    "Asi tambien es que se usan las funciones de pérdida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a978c59",
   "metadata": {},
   "source": [
    "El paquete de redes neuronales(nn) contiene varios módulos y funciones de pérdida que forman los componentes básicos de las redes neuronales profundas. Una lista completa con documentación está aquí <https://pytorch.org/docs/nn>_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7351f7",
   "metadata": {},
   "source": [
    "## 4. Actualización de pesos(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ddcb8",
   "metadata": {},
   "source": [
    "La regla de actualización más simple utilizada en la práctica es el **descenso de gradiente estocástico (SGD):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ba565",
   "metadata": {},
   "source": [
    "``weight = weight - learning_rate * gradient``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359d8ef0",
   "metadata": {},
   "source": [
    "Esto se puede implementar con simple codigo Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825f2b9",
   "metadata": {},
   "source": [
    "Sin embargo, a medida que se implementan mas redes neuronales es mejor usar varias reglas de actualización diferentes como SGD, Nesterov-SGD, Adam, RMSProp, etc. Para habilitar esto, creamos un paquete pequeño: **torch.optim** que implementa todos estos métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "071fd81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# cree su optimizador\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# en su loop de entreno:\n",
    "optimizer.zero_grad()   # zero los gradientes buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Haga la actualizacion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c021ccd",
   "metadata": {},
   "source": [
    "A partir de estos enunciados que repasamos se pueden crear bases solidas para seguir adentrandose en el mundo de las redes neuronales. Es recomendable ver los distintos tipos de NN y como se implementarian cada uno de los conceptos repasados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db5a21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
